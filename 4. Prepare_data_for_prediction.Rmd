---
title: "Prepare G2F for prediction"
author: "Jim Holland, Qiuyue Chen"
date: '2023-01-04'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'X:/g2fcompetition2022')
library(data.table)
library(tidyverse)
```

Creat directory for output
```{r}
if (!dir.exists("Prediction_Datasets")){
  dir.create("Prediction_Datasets")
}else{
  print("dir exists")
}
```

# Genotype data
First call a perl script to transform genotype as it is way too faster than dealing with elements of matrix in R.
The input (Genotype_Data_All_Years_filtered_MAF.05MR0biallelic_numeric.txt) are numericalized genotype calls from TASSEL (0 = homoz. minor, 0.5 = het, 1 = homoz. major). Convert to heterozygosity calls. The output (Genotype_Data_All_Years_filtered_MAF.05MR0biallelic_numeric_D.txt) are calls (0 = homoz, 1 = het).
```{r}
system("perl ./Transform_geno_to_heterozygosity_calls.pl Genotype_Data_All_Years_filtered_MAF.05MR0biallelic_numeric.txt")
```

```{r}
#geno = fread("./Genotype_Data_All_Years_filtered_MAF.05MR0biallelic_numeric.txt", skip = 2, header = F)
#markers = scan(text = readLines("./Genotype_Data_All_Years_filtered_MAF.05MR0biallelic_numeric.txt", n = 2), what = "", quiet = TRUE)
geno = fread("./Genotype_Data_All_Years_filtered_MAF.05MR0biallelic_numeric_D.txt", skip = 2, header = F)
markers = scan(text = readLines("./Genotype_Data_All_Years_filtered_MAF.05MR0biallelic_numeric_D.txt", n = 2), what = "", quiet = TRUE)
markers = markers[-c(1:2)] #first row of file is '<Numeric>', first element of 2nd row is 'Marker', get rid of that stuff
```


Extract the first column (hybrid names) 
```{r}
hybrids = unlist(geno[,1])
geno = geno[,-c(1)]
```

Transpose geno and attach row and column names
```{r}
colnames(geno) = markers
rownames(geno) = hybrids
```
These data are numericalized genotype calls from TASSEL (0 = homoz. minor, 0.5 = het, 1 = homoz. major). Convert to heterozygosity calls.


```{r}
#geno.d = setnafill(geno, fill = 0)
#geno.d[geno.d == 1] = 0
#geno.d[geno.d == 0.5] = 1
#This matrix is too big, so it is extremely slow to handle in R. So I used Perl to do this step, which only needs <2 min. 
geno.d = geno
dim(geno.d)
```

There is <1% missing calls here, but replace them with 0

Get the principal components of the D matrix, compute the percent variance associated with each PC
```{r}
PC.object = prcomp(geno.d)
PC.vars = PC.object$sdev^2
PC.pervars = PC.vars/sum(PC.vars)
```

How many PCs needed to capture 99% of the marker D variation?
```{r}
PC.cumsum = cumsum(PC.pervars)
PC.99 = PC.cumsum[PC.cumsum <= 0.99]
length(PC.99)
```
```{r}
plot(1:length(PC.cumsum), PC.cumsum)
```
Focus in on the region of the bend in percent variance explained
```{r}
plot(500:1341, PC.cumsum[500:1341])
```
How many markers to get 98% of variation?

```{r}
length(PC.cumsum[PC.cumsum <= 0.98])
```
So, we need to increase by 337 markers to get from 98 to 99% variation. If we need to reduce dimensionality, this would be effective.

Output the top 99% variation PCs to a file
```{r}
PC.scores = data.table(PC.object$x)[,1:length(PC.cumsum[PC.cumsum <= 0.99])]
rownames(PC.scores) = hybrids
fwrite(PC.scores, file = "./Prediction_Datasets/MarkerD_PCs_99perc.csv", row.names = TRUE)
```

After testing speed of building the matrices of GxE covariate interaction coefficients, it's clear that we cannot handle the very large dimensions. Have to reduce dimensions of D matrix to be able to capture all of the E covariate interactions. For now, let's try it with a much smaller set of PCs, capturing only 80% of the marker variation:
```{r}
PC.80 = PC.scores[,1:length(PC.cumsum[PC.cumsum <= 0.8])]
ncol(PC.80)
```

# Environmental data
Now read in the environmental covariables for both training and test sets
```{r}
trainE = fread("./Training_Data/6_Training_EC_Data_2014_2021.csv")
testE = fread("./Testing_Data/6_Testing_EC_Data_2022.csv")
```

Check that column names of the two environmental data sets are equal
```{r}
all(colnames(trainE) %in% colnames(testE))
```
Join the two environmental data sets and scale the EC variables
```{r}
EC = bind_rows(trainE, testE)
```

Some of the EC columns have no variance, still???
```{r}
Emat_var = apply(EC, 2, var)
badEs = which(Emat_var < 0.00001) #first column is Env and will return NA, that is ok here
badEs
```
Let's remove these
```{r}
EC = EC[, names(badEs):=NULL]
```

Scale the EC variables
```{r}
EC_scale = data.table(scale(EC[,2:ncol(EC)]))
EC[,2:ncol(EC)] = EC_scale
```

# Latitude and longitude
Get the environment latitude and longitude from the meta-data files
```{r}
meta.train = read.csv("./Training_Data/2_Training_Meta_Data_2014_2021.csv")
meta.test = read.csv("./Testing_Data/2_Testing_Meta_Data_2022.csv")
```

All we can really use from meta-data are latitude and longitude
```{r}
meta.train = meta.train %>%
  select(Env, City, starts_with("Weather_Station_L")) %>%
  rename(Latitude = contains("Latitude"),
         Longitude = contains("Longitude"))

meta.test = meta.test %>%
  select(Env, City, starts_with("Weather_Station_L")) %>%
  rename(Latitude = contains("Latitude"),
         Longitude = contains("Longitude"))

```

Need to fill in coordinates for missing locations:
```{r}
missing.sites = rbind(meta.train, meta.test) %>% 
  filter(is.na(Latitude)) %>%
  #filter(!duplicated(City)) %>%
  select(Env, City)
```

Get the mean coordinates of those cities when they were reported, some are also missing City label, so need to input those missing City values first
```{r}
missing.sites = missing.sites %>%
  mutate(City = case_when(
    City == "" & grepl("IL", Env) ~ "Champaign",
    City == "" & grepl("IN", Env) ~ "West Lafayette",
    City == "" & grepl("TX", Env) ~ "Lubbock",
    T ~ City))

missing.site.coords = 
rbind(meta.train, meta.test) %>% filter(City %in% missing.sites$City) %>%
  group_by(City) %>%
  summarize(Latitude.mn = mean(Latitude, na.rm = T),
            Longitude.mn = mean(Longitude, na.rm = T))
missing.site.coords
```


Only 2 cities are missing: Lubbockand Wahoo. Input them by hand:
```{r}
hand.coords = data.frame(City = c("Lubbock", "Wahoo"), 
                         Latitude.mn = c(33.577863, 41.211502),
                         Longitude.mn = c(-101.855166, -96.621260))
missing.site.coords = missing.site.coords %>%
  filter(!is.na(Latitude.mn))

missing.site.coords = rbind(missing.site.coords, hand.coords) 

missing.site.coords = merge(missing.sites, missing.site.coords) %>%
  select(-City)
```

Merge the missing coordinates with the meta.data
```{r}
meta.all = rbind(meta.train, meta.test) %>%
  left_join(missing.site.coords, by = "Env") %>%
  mutate(Latitude = ifelse(is.na(Latitude), Latitude.mn, Latitude),
         Longitude = ifelse(is.na(Longitude), Longitude.mn, Longitude)) %>%
  select(-Latitude.mn, -Longitude.mn)
```

Keep the lat and long data only from the environments in the train and test sets, scale, and make quadrative versions as well.
```{r}
envs = unique(EC[, Env])
meta.filt = meta.all %>% filter(Env %in% envs) %>%
  select(-City) %>%
  mutate(Latitude = as.numeric(scale(Latitude)),
         Longitude = as.numeric(scale(Longitude)),
         Lat2 = Latitude^2,
         Lon2 = Longitude^2)
```

Combine latitude and longitude with the other EC variables.
```{r}
EC = merge(meta.filt, EC, by = "Env")
```

Check correlations among EC variables, we could drop some variables with high correlations to others. Adapted from stackoverflow https://stackoverflow.com/questions/18275639/remove-highly-correlated-variables
answer from user David
```{r}
EC.cor = cor(EC[,2:ncol(EC)])
#make the upper triangular
EC.cor.up = EC.cor
EC.cor.up[upper.tri(EC.cor.up)] = 0
diag(EC.cor.up) = 0

highly.corr.cols = apply(EC.cor.up, 2, function(x) any(abs(x) > 0.97, na.rm = T))
highly.corr.names = colnames(EC_scale)[highly.corr.cols]
EC.sub = EC[, !names(EC) %in% c(highly.corr.names)] #EC is a regular data frame
ncol(EC.sub)
```
And one of these columns of EC is still the Env indicator column.

Column dimensions of products of top 80% D PCs and reduced E matrices:
```{r}
length(PC.80) * ncol(EC.sub[,-c(1)])
```
That's a lot easier to deal with, maybe still too much. keep that in mind.


Also, turns out that some environments are missing all the EC data???
```{r}
Emiss2= apply(EC, 2, FUN = function(x) sum(is.na(x)))
summary(Emiss2)
```

No data are missing in EC. But it turns out that many environments are completely missing from EC, so when we later merge with trait data, they are lost. 
A major problem is that two of them are in the test set! IAH1_2022 and NCH1_2022!
For these cases, all we can do is generate mean EC values at those locations over the other available years.
```{r}
NCH1.mean.EC = apply(EC.sub[grepl("NC", EC.sub$Env), 2:ncol(EC.sub)], 2, mean)
NCH1.EC.df = data.frame(Env = "NCH1_2022", as.list(NCH1.mean.EC))
```

In the case of IAH1_2022, we can average over the other three locations in IA from same year:
```{r}
IA22.mean.EC = apply(EC.sub[grepl("IAH._2022", EC.sub$Env), 2:ncol(EC.sub)], 2, mean)
IA22.EC.df = data.frame(Env = "IAH1_2022", as.list(IA22.mean.EC))
```

Add in the imputed EC variables for the two missing test environments
```{r}
EC.sub = bind_rows(EC.sub, NCH1.EC.df, IA22.EC.df)
```


# Trait data
Get the yield means from each training environment, outer merge with the EC data
```{r}
yield.train = fread("./Stage1_analysis/G2F 2021 Stage 1 yield BLUEs no filtering.csv")
colnames(yield.train)[colnames(yield.train) == "predicted.value"] = "yield"
yield.train$weight = 1/(yield.train$std.error^2)
```

Get the combinations of hybrids and environments in the testing data set
```{r}
test.template = fread("./Testing_Data/1_Submission_Template_2022.csv")
colnames(test.template)[colnames(test.template) == "Yield_Mg_ha"] = "yield"
test.template$Year = 2022
```

Combine the training and test sets of hybrid-env combinations:
```{r}
yield.train_test = bind_rows(yield.train, test.template)
```

Merge with the environmental predictors
```{r}
yield.EC = merge(yield.train_test, EC.sub, by = "Env")
```

Merge with the top 80% of marker D PCs
```{r}
PC.80$Hybrid = hybrids #data.table merge requires this to be a real column not a row name
yield.EC.geno = merge(yield.EC, PC.80, by = "Hybrid")
```

Write out this version of the data frame to use in computing environment means
```{r}
fwrite(yield.EC.geno, file = "./Prediction_Datasets/Yield_Geno_EC.csv")
```


Compute environment means
```{r}
mean.mod = lm(yield ~ as.factor(Env), data = yield.EC.geno)
env.fx = mean.mod$coefficients[-c(1)]
names(env.fx) = sub("as.factor(Env)", "", names(env.fx), fixed = T)
env.fx.df = data.frame(Env = names(env.fx), yield = env.fx)
```


Compute environment means adjusted for hybrid effects
```{r}
ptm <- proc.time()
mean.adj.mod = lm(yield ~ as.factor(Env) + as.factor(Hybrid), data = yield.EC.geno)
proc.time() - ptm
```
   user  system elapsed 
2140.86    4.58 2172.95

```{r}
env.fx.adj = mean.adj.mod$coefficients
env.fx.adj = env.fx.adj[grepl("as.factor(Env)", names(env.fx.adj), fixed = T)]
names(env.fx.adj) = sub("as.factor(Env)", "", names(env.fx.adj), fixed = T)
env.fx.adj.df = data.frame(Env = names(env.fx.adj), yield = env.fx.adj)
```

Just for kicks, plot the unadjusted vs adjusted environment effects:
```{r}
plot(env.fx.df$yield, env.fx.adj.df$yield)
```
The adjustments are small, that's probably a good thing. Let's use the adjusted environment means and select the 200 EC variables with lower p-values.

```{r}
EC.sub.mn = merge(env.fx.adj.df, EC.sub, by = "Env") #164 environments in test set with both EC variables and yield mean
pvalEC = function(x, dt = EC.sub.mn){
  mod = lm(as.formula(paste('yield ~ ', x)), data = dt)
  p = summary(mod)$coefficients[2,"Pr(>|t|)"]
}
EC.pvals = sapply(names(EC.sub.mn)[-c(1:2)], pvalEC, simplify = T)
hist(EC.pvals)
```
Looks like well over 50% have pvalue < 0.5  
Let's check what the threshold pvalue for lowest 200 is:
```{r}
bestECs = names(sort(EC.pvals, decreasing = F)[1:200])
max(EC.pvals[bestECs])
```
It's more stringent than I would like, but we need to cut the dimensions down somehow, so let's use this.
```{r}
selectCols = c("Env", bestECs)
EC.sub2 = EC.sub[, selectCols]
```

Now make a new version of the data set that includes only the smallest set of predictors (107 PCs + 200 ECs) and ONLY the training set, so we can reduce the data set size for initial model fitting and testing. When we move to production we will add stuff back in.
```{r}
dropCols = names(EC.pvals)[!names(EC.pvals) %in% bestECs]
practiceDF = yield.EC.geno[, c(dropCols):=NULL]
practiceDF = yield.EC.geno[yield.EC.geno$Year != 2022,]
fwrite(practiceDF, file = "./Prediction_Datasets/practiceDF.csv")
```

# Generate the GxE covariates
Requires a loop to multiply the marker matrix part by each column of the EC matrix part.  
This is a weird matrix calculation and I can't find a shortcut.
At the moment I want to make 107 PCs by 100 ECs = 10,700 GxE covariate columns.
Select the 100 ECs with lowest pvalue in regression of yield on each EC.
```{r}
Dcols = grep("PC", colnames(yield.EC.geno), value = T) #marker D column names
EC100 = names(sort(EC.pvals, decreasing = F)[1:100])
#max(EC.pvals[EC100]) #[1] 0.04818421]

Dmat = as.matrix(yield.EC.geno[,..Dcols])
Emat = as.matrix(yield.EC.geno[,..EC100])
```

##testing different ways of building the GxE matrices
most were too slow or memory hogging

To save some memory, do this step in ten parts, each time doing 10% of the interaction variables and writing to disk the intermediate file.
This is made harder because we are generated new columns, rather than new rows, of the data set. We cannot easily concatenate files by columns.  (Maybe paste command in linux will do this?)
Perhaps it will work to write the transposed matrix to disk, concatenate the ten transpose files using command line operations, then see if we can read it all back in. Finally, we need to transpose last time, perhaps this function will work?: wgcna::transposeBigData()
For the moment, let's skip the transposing step and write out the matrices in the desired orientation, then we can see if paste command in linux will work to concatenate by columns. Make function to generate the column products
```{r}
make_interactions = function(col, mat1 = Emat, mat2 = Dmat, cnames1 = colnames(Emat)){
  GxE_tmp <- mat1 * mat2[, col]
  colnames(GxE_tmp) <- paste0(cnames1, "_PC", col)
  return(GxE_tmp)  
}
```
  
```{r}
#N = ncol(Dmat)
# for (i in 1:10){
#   maxi = min(ceiling(N/10)*i, N)
#   mini = (maxi - (ceiling(N/10)-1))
#   GxE.list = lapply(c(mini:maxi), FUN = function(x) make_interactions(x))
#   GxE = do.call(cbind, GxE.list)
#   fwrite(GxE, file = paste0("./GxE_matrices/GxE", i))
#   rm(GxE.list, GxE)
# }
#GxE.list = lapply(1:N, FUN = function(x) make_interactions(x))
#GxE = do.call(cbind, GxE.list)

#fwrite(GxE, file = "./Prediction_Datasets/GxE_107Dx100EC.csv")
```
That file size is still enormous! 14GB!  
So, here also make a smaller version with 1000GxE covariates from 10 PCs x 100 ECs
```{r}
N = 10
GxE.list_10 = lapply(1:N, FUN = function(x) make_interactions(x))
GxE_10 = do.call(cbind, GxE.list_10)
fwrite(GxE_10, file = "./Prediction_Datasets/GxE_10Dx100EC.csv")
```
That is 160Mb.  
  
Now combine the 10Dx100EC with all 477 ECs that passed the correlation threshold and 500 D PCs. 500 PCs captures how much of D variance?
```{r}
PC.cumsum[500]
```
[1] 0.9394244
```{r}
PC.500 = PC.scores[,1:500]
PC.500$Hybrid = hybrids

yield.EC477.geno500 = merge(yield.EC, PC.500, by = "Hybrid")

yield.EC477.geno500.ge1k = cbind(yield.EC477.geno500, GxE_10)

fwrite(yield.EC477.geno500, file = "./Prediction_Datasets/G2F_EC477_geno500.csv")
fwrite(yield.EC477.geno500.ge1k, file = "./Prediction_Datasets/G2F_EC477_geno500_ge1k.csv")
```
Do some checking to make sure correct columns are written out and read back in.
test1 = fread("./Prediction_Datasets/G2F_EC477_geno500.csv")
ncol(test1)
test2 = fread("./Prediction_Datasets/G2F_EC477_geno500_ge1k.csv")
ncol(test2)




The following part is to generate another data set with a larger number of markers.
First call a perl script to transform genotype as it is way too faster than dealing with elements of matrix in R.
The input (Genotype_Data_All_Years_filtered_MAF.05MR.01biallelic_numeric.txt) are numericalized genotype calls from TASSEL (0 = homoz. minor, 0.5 = het, 1 = homoz. major). Convert to heterozygosity calls. The output (Genotype_Data_All_Years_filtered_MAF.05MR.01biallelic_numeric_D.txt) are calls (0 = homoz, 1 = het).
```{r}
system("perl ./Transform_geno_to_heterozygosity_calls.pl Genotype_Data_All_Years_filtered_MAF.05MR.01biallelic_numeric.txt")
```

# Genotype data
```{r}
#geno = fread("./Genotype_Data_All_Years_filtered_MAF.05MR.01biallelic_numeric.txt", skip = 2, header = F)
#markers = scan(text = readLines("./Genotype_Data_All_Years_filtered_MAF.05MR.01biallelic_numeric.txt", n = 2), what = "", quiet = TRUE)
geno = fread("./Genotype_Data_All_Years_filtered_MAF.05MR.01biallelic_numeric_D.txt", skip = 2, header = F)
markers = scan(text = readLines("./Genotype_Data_All_Years_filtered_MAF.05MR.01biallelic_numeric_D.txt", n = 2), what = "", quiet = TRUE)
markers = markers[-c(1:2)] #first row of file is '<Numeric>', first element of 2nd row is 'Marker', get rid of that stuff
```

Extract the first column (hybrid names) 
```{r}
hybrids = unlist(geno[,1])
geno = geno[,-c(1)]
```

Transpose geno and attach row and column names
```{r}
colnames(geno) = markers
rownames(geno) = hybrids
```

```{r}
#geno.d = setnafill(geno, fill = 0)
#geno.d[geno.d == 1] = 0
#geno.d[geno.d == 0.5] = 1
#This matrix is too big, so it is extremely slow to handle in R. So I used Perl to do this step, which only needs <2 min. 
geno.d = geno
dim(geno.d)
#[1]   4928 234940
```

Get the principal components of the D matrix, compute the percent variance associated with each PC
```{r}
PC.object = prcomp(geno.d)
PC.vars = PC.object$sdev^2
PC.pervars = PC.vars/sum(PC.vars)
```

How many PCs needed to capture 99% of the marker D variation?
```{r}
PC.cumsum = cumsum(PC.pervars)
PC.99 = PC.cumsum[PC.cumsum <= 0.99]
length(PC.99)
#[1] 3637
```

```{r}
plot(1:length(PC.cumsum), PC.cumsum)
```

```{r}
length(PC.cumsum[PC.cumsum <= 0.98])
#[1] 3116
length(PC.cumsum[PC.cumsum <= 0.90])
#[1] 1472 
length(PC.cumsum[PC.cumsum <= 0.80])
#[1] 724
```
OK, let's use the top 90% variation PCs for analysis, but still output the top 99% variation PCs to a file
```{r}
PC.scores = data.table(PC.object$x)[,1:length(PC.cumsum[PC.cumsum <= 0.99])]
rownames(PC.scores) = hybrids
fwrite(PC.scores, file = "./Prediction_Datasets/G2F_FullMarkerD_PCs_99perc.csv", row.names = TRUE)
```

Read in the dat set generated from the 10k markers to pull out the same ECs
```{r}
df = fread("./Prediction_Datasets/G2F_EC477_geno500_ge1k.csv")
df.yield.EC = df[,1:485]
```

Get the top 90% variation PCs
```{r}
PC = fread("./Prediction_Datasets/G2F_FullMarkerD_PCs_99perc.csv")
PC.90 = PC[,1:1473]
```

```{r}
df.new = merge(df.yield.EC, PC.90, by = "Hybrid") ##

GxE.list = names(df)[986:1985] #get names of the top ge1k (100ECsx10PCs) 
GxE.list2 = do.call(rbind, strsplit(GxE.list,"_PC"))[,1] #split out names of ECs
GxE.list3 = c(unique(GxE.list2), names(PC.90)[2:11]) #names of 100 ECs and top 10 PCs
GxE.list3

PC10.EC100 = df.new[, ..GxE.list3]
fwrite(PC10.EC100 , file = "./Prediction_Datasets/G2F_FullMarker_PC10_EC100.csv")
```

call a perl script to calculate top 100ECsx10PCs as it is way too faster than calculation in R.
The input file is G2F_FullMarker_PC10_EC100.csv, and the output file is G2F_FullMarker_PC10xEC100.csv
```{r}
system("perl ./Calculate_GxE_covariates.pl")
```

Generate the final file, which has 477 ECs, 1472 PCs and 1000 GxE covariates from 10 PCs x 100 ECs
```{r}
GxE = fread("./Prediction_Datasets/G2F_FullMarker_PC10xEC100.csv")
df.final = cbind(df.new, GxE)
fwrite(df.final , file = "./Prediction_Datasets/G2F_FullMarker_EC477_geno1472_ge1k.csv")
```

Also write out a version without GxE covariates
```{r}
fwrite(df.new , file = "./Prediction_Datasets/G2F_FullMarker_EC477_geno1472.csv")
```



